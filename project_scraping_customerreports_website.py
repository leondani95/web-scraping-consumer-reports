# -*- coding: utf-8 -*-
"""Project Scraping CustomerReports Website.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AAGhbzR6XbTdvsz_BxCcUsQPguzGXX1n

## **PART 1**
"""

!pip install fake_useragent

import requests
from fake_useragent import UserAgent
from bs4 import BeautifulSoup

url = 'https://www.consumerreports.org/cro/a-to-z-index/products/index.htm'
file_name = 'consumer_reports.txt'

page = requests.get(url, headers={'User-Agent': user_agent.chrome})

if page.status_code == 200:
  with open(file_name, 'w', encoding='utf-8') as file:
    file.write(page.text)

def read_file():
  file = open('consumer_reports.txt')
  data = file.read()
  file.close()
  return data

soup = BeautifulSoup(read_file(),'lxml')

all_divs = soup.find_all('div',attrs={'class':'crux-body-copy'})

for div in all_divs:
  if div.a:
    print(div.a.string.strip())

product_names = [div.a.string if div.a else '' for div in all_divs] #List Comprehension

for product in product_names:
  print(product)

products = {}

product_links = [urljoin(url, div.a['href']) for div in soup.find_all('div', class_='crux-body-copy') if div.a and 'href' in div.a.attrs]

for link in product_links:
  print(link)

from urllib.parse import urljoin

product_links = [
    urljoin(url, div.a['href'])
    for div in soup.find_all('div', class_='crux-body-copy')
    if div.a and 'href' in div.a.attrs
]

for link in product_links:
  print(link)

products = {
    div.a.string.strip() if div.a else '': urljoin(url, div.a['href'])
    for div in soup.find_all('div', class_='crux-body-copy')
    if div.a and 'href' in div.a.attrs
}

for key, value in products.items():
  print(key,' ---> ', value)